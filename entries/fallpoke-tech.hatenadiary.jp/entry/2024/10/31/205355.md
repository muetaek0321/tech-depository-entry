---
Title: HaggingFaceの物体検出モデルを試してみようの回
Category:
- Python
- Pytorch
- 機械学習
- 画像処理
Date: 2024-10-31T20:53:55+09:00
URL: https://fallpoke-tech.hatenadiary.jp/entry/2024/10/31/205355
EditURL: https://blog.hatena.ne.jp/fallpoke/fallpoke-tech.hatenadiary.jp/atom/entry/6802418398300410328
---

お疲れ様です。

HuggingFace（transformersライブラリ）から利用できる物体検出のDeepLearningモデルを試してみたのでその紹介です。   
物体検出についてはざっくりというと画像内の物体を矩形（Bounding Box）で囲って検出するものになります。  
下図の例では人間の顔を検出しています。  
[f:id:fallpoke:20241031193523p:plain]
   
作成したソースコードはテンプレート化して公開しているので気になる方はこちらもご参照ください。    

[https://github.com/muetaek0321/HF-ObjectDetection-Models:embed:cite]

さて、HuggingFaceといえば自然言語処理のモデルが多いイメージかなと思いますが、VisionTransformerなどtransfomerアーキテクチャが使用された画像処理のモデルも多くあります。  
transfomerを使った物体検出モデルでもおそらく一番有名であろう**DETR**を実装しました。
以下はHuggingFaceのDETRの紹介ページですがかなり参考になりました。DETRについての詳細な説明もこちらを見ていただければと思います。

[https://huggingface.co/docs/transformers/ja/model_doc/detr:embed:cite]

一応こちらにも説明を…（生成AIに書かせたやつです笑）

>DETR (DEtection TRansformer) は、Facebook AI Research が開発した物体検出モデルで、Transformer アーキテクチャを使用する点が特徴です。従来の物体検出モデルは、複数の手法（アンカーボックス生成やNMSなど）を組み合わせる必要がありましたが、DETR はこれらを不要にし、シンプルなエンドツーエンドの学習を実現しています。
>
>DETRは主に以下の2つのパートから構成されます：
>
>1. **CNNバックボーン**：ResNet などのCNNを用いて画像特徴量を抽出します。
>2. **Transformer エンコーダ・デコーダ**：エンコーダで画像の特徴をエンコードし、デコーダがクエリベースで物体の位置とカテゴリを予測します。
>
>このアプローチにより、アンカーボックスや後処理の必要がなく、訓練が単純化され、様々な検出タスクで高い精度が得られるようになっています。

実装に関してはHuggingFaceモデルを使用した解説はほぼないのでライブラリのソースコードを解読することが多かったです。  
以前作成した[AlbumentationsのDataAugmentation](https://fallpoke-tech.hatenadiary.jp/entry/2024/09/16/182637)を使いたかったのですが、これをうまく利用しようとするとめちゃくちゃ大変でした…。
参考になったサイトは以下の2サイト。とはいっても公式のチュートリアルとtorch.hubから読み込めるpretrainedモデルを使用した実装なので基本的には参考程度でした。

[https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR:embed:cite]

[https://zenn.dev/aidemy/articles/30f10cf2c48e6a:embed:cite]

紆余曲折あって実装したのが最初に公開したGitHubリポジトリにあります。  
とりあえず完成したので実際に学習を回してみようということで、VOC2012の物体検出データセットを使ってお試しの学習をしてみました。
<figure class="figure-image figure-image-fotolife" title="学習曲線">[f:id:fallpoke:20241031200641p:plain]<figcaption>学習曲線</figcaption></figure>
訓練データが5000枚、検証データが2500枚で100epoch回してみた結果です。  
ベンチマーク用のデータセットなのでepoch数が少ないとあまり学習が進まないのでしょうか…。それ以前にあまり安定していないのも気になりますね。  パラメータチューニングはやっていないのでそれも影響はありそうです。  
検出結果をいくつか載せておきます。ここには物体を検出できた結果のみを載せていますが、そもそも未検出の画像も結構ありました。
[f:id:fallpoke:20241031201231p:plain]
[f:id:fallpoke:20241031201254p:plain]
[f:id:fallpoke:20241031201314p:plain]

ちなみにDETRの改良版として**Deformable-DETR**というモデルもあり、そちらも使えるように実装はしています。  
ただVOC2012データセットで学習するとめちゃくちゃ時間がかかるため、学習は断念しました…。1epoch学習を完了するのに2時間はさすがに…笑
[f:id:fallpoke:20241031201932p:plain]
モデルの比較もしたいということで現在簡単なデータセットを作成中です。  
またそちらも結果がでたら記事にしたいなと思っています。

