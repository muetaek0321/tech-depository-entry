---
Title: セグメンテーションモデルMask2Formerについて調べたまとめ
Category:
- Python
- Pytorch
- 画像処理
Date: 2025-01-31T21:55:13+09:00
URL: https://fallpoke-tech.hatenadiary.jp/entry/2025/01/31/215513
EditURL: https://blog.hatena.ne.jp/fallpoke/fallpoke-tech.hatenadiary.jp/atom/entry/6802418398324286842
---

お疲れ様です。  
商用利用可能で性能のよいセグメンテーションモデルが必要になったので調査した内容をメモに残しておきます。

[:contents]

### リンク
- 論文
[https://arxiv.org/abs/2112.01527:embed:cite]

- 論文解説
[https://speakerdeck.com/koharite/mask2former:embed:cite]

- 公式実装
[https://github.com/facebookresearch/Mask2Former:embed:cite]


### 概要
以下は生成AI（Gemini）に聞いた内容。
> Mask2Former は、**様々なセグメンテーションタスクを統一的なフレームワークで実行できる**、非常に強力で汎用性の高い最新のセグメンテーションモデルです。  2022年に発表され、瞬く間にセグメンテーション分野で注目を集めました。
> 
> **Mask2Former の重要なポイント**
> 
> 1. **統一されたアプローチ (Unified Approach):**
>    * Mask2Former の最大の特徴は、**インスタンスセグメンテーション、セマンティックセグメンテーション、パノプティックセグメンテーション**という、異なるセグメンテーションタスクを **同じモデルアーキテクチャと学習方法** で扱える点です。
>    * 従来のセグメンテーションモデルは、タスクごとに異なる構造や学習方法が必要でしたが、Mask2Former はこれを統一し、**汎用的なセグメンテーションモデル** を実現しました。
> 
> 2. **マスク分類 (Mask Classification) の考え方:**
>    * Mask2Former は、セグメンテーションを **ピクセルごとの分類ではなく、マスクの分類** として捉えます。
>    * 画像全体から抽出された特徴量をもとに、**各セグメント (オブジェクトや領域) に対応するマスクを直接予測** します。
>    * このアプローチにより、従来のピクセル単位の分類に比べて、**文脈情報をより効果的に活用** し、**より整合性の取れたセグメンテーション結果** を得ることができます。
> 
> 3. **Transformer デコーダの活用:**
>    * Mask2Former は、Transformer アーキテクチャの **デコーダ** 部分を効果的に活用しています。
>    * Transformer デコーダは、**グローバルな文脈情報** を捉えるのが得意であり、画像全体の情報を考慮してセグメンテーションを行うことができます。
>    * 特に、**複数のオブジェクトが密集しているシーン** や、**複雑な形状のオブジェクト** のセグメンテーションにおいて、高い性能を発揮します。
> 
> 4. **マルチスケール特徴の統合:**
>    * 画像の様々なスケールの情報を捉えるために、**マルチスケール特徴** を効果的に統合しています。
>    * これにより、**小さなオブジェクトから大きなオブジェクトまで**、様々なサイズのオブジェクトを精度良くセグメンテーションできます。
> 
> 5. **高い性能:**
>    * Mask2Former は、インスタンスセグメンテーション、セマンティックセグメンテーション、パノプティックセグメンテーションの各タスクにおいて、**最先端 (State-of-the-Art) の性能** を達成しています。
>    * 特に、**大規模なデータセット** で学習することで、その性能を最大限に引き出すことができます。
> 
> **Mask2Former のアーキテクチャの概要 (非常に簡略化)**
> 
> 1. **バックボーンネットワーク (例: ResNet, Swin Transformer):**  入力画像から特徴量を抽出します。
> 2. **ピクセルデコーダ (Pixel Decoder):** バックボーンネットワークから抽出されたマルチスケール特徴を統合し、高解像度の特徴マップを生成します。
> 3. **Transformer デコーダ (Transformer Decoder):**
>    * 特徴マップと学習可能な **クエリ** を入力として受け取ります。
>    * Transformer デコーダが、クエリと特徴マップの間でアテンション機構を用いて相互作用し、**各クエリがセグメント (オブジェクトや領域) を表現する** ように学習されます。
>    * 各クエリに対応する **マスク予測** と **クラス予測** が出力されます。
> 4. **マスク分類器 (Mask Classifier):**  予測されたマスクとクラスを用いて、最終的なセグメンテーション結果を生成します。

上記には記載はないが、Mask2Formerは**MITライセンス**なので商用利用可能。  
（ちなみに他にNVIDIAの[SegFormer](https://github.com/NVlabs/SegFormer)が候補にあったがこちらはライセンス的に商用利用不可。）

### 実装
ざっと調べた感じdetectron2やmmdetで実装されているのが多そう。  
使い慣れたHuggingFaceからも使えるので、今回はこれを利用して推論処理だけ実装してみます。
[https://huggingface.co/docs/transformers/model_doc/mask2former:embed:cite]

調べると推論だけならサンプルコードもあります。  
[https://huggingface.co/facebook/mask2former-swin-small-coco-instance:embed:cite]

出力するとこんな感じ。
[f:id:fallpoke:20250131214225p:plain]

ざっと見た感じ指定するモデルの名前によってセグメンテーションの種類を切り替える感じのようです。  
使用したサンプルコードのモデルは”facebook/mask2former-swin-small-coco-**instance**”だったのでインスタンスセグメンテーションになっていますね。  
他を調べるとセマンティックセグメンテーション（”facebook/mask2former-swin-large-cityscapes-**semantic**”など）やパノプティックセグメンテーション（”facebook/mask2former-swin-large-coco-**panoptic**”）も確認できました。

ほぼ、上記のサンプルと同じですがソースコードはこちらに残しておきます。
[https://github.com/muetaek0321/tech-depository/tree/main/post_20250131:embed:cite]


